{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task #1 - Teoría"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silvia Illescas y Michelle Mejía"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 1 (preguntas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué es el temporal difference learning y en qué se diferencia de los métodos tradicionales de aprendizaje\n",
    "supervisado? Explique el concepto de \"error de diferencia temporal\" y su papel en los algoritmos de\n",
    "aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temporal Difference Learning (TD Learning)** es una técnica de **aprendizaje por refuerzo** que permite a un agente aprender a tomar decisiones basándose en experiencias previas sin necesidad de conocer de antemano todas las recompensas futuras. A diferencia de los métodos tradicionales de **aprendizaje supervisado**, que requieren un conjunto de datos etiquetados y ajustan los parámetros basándose en errores calculados con la respuesta correcta, TD Learning aprende de manera **incremental** mientras interactúa con su entorno, sin esperar el resultado final de una secuencia de acciones. Esto lo hace más eficiente en entornos dinámicos y de aprendizaje continuo, donde los resultados a largo plazo son inciertos.\n",
    "\n",
    "El **error de diferencia temporal (TD error)** es la diferencia entre la estimación actual de la función de valor y una nueva estimación obtenida tras observar la siguiente recompensa y el estado resultante. \n",
    "\n",
    "Este error permite ajustar la función de valor progresivamente, acercándola a la realidad sin necesidad de esperar el desenlace completo de una acción. En algoritmos como **Q-learning y SARSA**, el TD error es fundamental para actualizar los valores de estado-acción, lo que permite que el agente aprenda estrategias óptimas en entornos con incertidumbre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. En el contexto de los juegos simultáneos, ¿cómo toman decisiones los jugadores sin conocer las acciones\n",
    "de sus oponentes? De un ejemplo de un escenario del mundo real que pueda modelarse como un juego\n",
    "simultáneo y discuta las estrategias que los jugadores podrían emplear en tal situación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los **juegos simultáneos**, los jugadores toman decisiones sin conocer las acciones de sus oponentes, lo que los obliga a basarse en estrategias que maximicen su beneficio sin información completa. Para ello, pueden optar por **estrategias puras**, donde eligen una acción fija, o **estrategias mixtas**, donde asignan probabilidades a diferentes opciones para hacer su decisión menos predecible. En estos juegos, los jugadores suelen analizar los posibles resultados de sus acciones en función de las decisiones que podrían tomar sus rivales, utilizando herramientas como la teoría de juegos, donde cada jugador selecciona una estrategia óptima bajo la suposición de que el otro también lo hará.\n",
    "\n",
    "Un ejemplo real es la competencia entre aerolíneas para fijar precios de boletos. Si American Airlines y Delta deben decidir si bajan sus tarifas sin conocer la decisión de la otra, pueden analizar patrones de precios pasados o alternar entre estrategias para evitar pérdidas y maximizar ganancias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ¿Qué distingue los juegos de suma cero de los juegos de suma cero y cómo afecta esta diferencia al\n",
    "proceso de toma de decisiones de los jugadores? Proporcione al menos un ejemplo de juegos que entren\n",
    "en la categoría de juegos de no suma cero y discuta las consideraciones estratégicas únicas involucradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los **juegos de suma cero** son aquellos donde la ganancia de un jugador equivale exactamente a la pérdida de otro, es decir, lo que uno gana, el otro lo pierde. En estos juegos, los jugadores adoptan estrategias competitivas como **minimax**, tratando de minimizar la máxima ganancia del oponente. Un ejemplo clásico es el **ajedrez**, donde un jugador solo gana si el otro pierde.  \n",
    "\n",
    "En contraste, los **juegos de no suma cero** permiten que ambos jugadores ganen o pierdan simultáneamente, lo que abre espacio para la cooperación. Un ejemplo es la **negociación salarial entre empleados y una empresa**: si llegan a un acuerdo justo, ambas partes pueden beneficiarse (los empleados obtienen mejores salarios y la empresa retiene talento). En estos casos, las estrategias incluyen **cooperación, negociación y repetición de interacciones** para generar confianza y maximizar beneficios conjuntos en lugar de centrarse solo en la competencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ¿Cómo se aplica el concepto de equilibrio de Nash a los juegos simultáneos? Explicar cómo el equilibrio de\n",
    "Nash representa una solución estable en la que ningún jugador tiene un incentivo para desviarse\n",
    "unilateralmente de la estrategia elegida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El **equilibrio de Nash** en juegos simultáneos es una situación en la que ningún jugador tiene incentivos para cambiar unilateralmente su estrategia, dado que cualquier desviación empeoraría su resultado o lo dejaría igual. En estos juegos, los jugadores toman decisiones sin conocer las elecciones de los demás y buscan estrategias óptimas asumiendo que los oponentes harán lo mismo.  \n",
    "\n",
    "Un ejemplo es la competencia entre McDonald's y Burger King, que deben decidir si abrir una sucursal en una nueva ubicación sin conocer la decisión del otro. Si ambas abren, obtienen ganancias moderadas debido a la competencia; si solo una abre, esta obtiene altos beneficios mientras la otra no gana nada; y si ninguna abre, ambas pierden la oportunidad de captar clientes. Si el equilibrio de Nash indica que ambas deben abrir, ninguna tiene incentivo para cambiar su decisión, ya que hacerlo reduciría su beneficio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Discuta la aplicación del temporal difference learning en el modelado y optimización de procesos de toma\n",
    "de decisiones en entornos dinámicos. ¿Cómo maneja el temporal difference learning el equilibrio entre\n",
    "exploración y explotación y cuáles son algunos de los desafíos asociados con su implementación en la\n",
    "práctica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El **Temporal Difference Learning (TD Learning)** se aplica en el modelado y optimización de procesos de toma de decisiones en entornos dinámicos mediante la actualización iterativa de estimaciones de valores de estado basadas en la experiencia obtenida en el tiempo. Se usa en problemas como el aprendizaje de estrategias óptimas en juegos, control de robots y toma de decisiones financieras, donde los resultados no son inmediatos y deben aprenderse progresivamente. TD Learning equilibra la **exploración y explotación** mediante estrategias como **ε-greedy**, donde el agente explora nuevas acciones con cierta probabilidad (ε) y el resto del tiempo elige la acción con mayor recompensa esperada, evitando quedarse atrapado en soluciones subóptimas. Sin embargo, su implementación presenta desafíos como la elección de una tasa de aprendizaje adecuada, la posible convergencia lenta en entornos complejos y la necesidad de una representación eficiente del estado, especialmente cuando se usa en problemas de alta dimensionalidad donde se requieren métodos como redes neuronales para aproximar funciones de valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 2 (discusión vídeo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Qué hace su agente entrenando con TD learning a nivel general\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente basado en Temporal Difference Learning (TD Learning) aprende a jugar Connect Four mejorando progresivamente su estrategia mediante la actualización de valores de estado-acción. Durante el entrenamiento, el agente juega miles de partidas contra sí mismo y ajusta su política en función de la recompensa obtenida tras cada movimiento. Usa una combinación de exploración (para probar nuevas jugadas) y explotación (para seguir las mejores jugadas aprendidas). A medida que juega más partidas, mejora su capacidad de anticipar movimientos ganadores y bloquear jugadas del oponente, optimizando sus decisiones en cada turno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explique por qué ganó más veces el agente que ganó. ¿Cómo afectó el tener o no esta estrategia al agente\n",
    "que ganó?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente Minimax con poda alfa-beta ganó más partidas porque utiliza una estrategia de búsqueda exhaustiva y toma decisiones óptimas dentro de la profundidad establecida. A diferencia del TD Learning, que aún está en proceso de aprendizaje y depende de la exploración y la función de valor estimada, Minimax evalúa todas las posibles jugadas dentro de su alcance y selecciona la mejor opción con base en reglas estrictas.\n",
    "\n",
    "La estrategia de poda alfa-beta mejoró la eficiencia del Minimax, permitiéndole descartar ramas irrelevantes en la búsqueda y tomar decisiones más rápido sin perder precisión. Esto le dio una ventaja adicional sobre el TD Learning, que todavía está optimizando su función de valor a partir de la experiencia y requiere más iteraciones para alcanzar un desempeño competitivo.\n",
    "\n",
    "En el caso de TD Learning vs. TD Learning, la diferencia en victorias sugiere que uno de los agentes pudo haber explorado mejor ciertos patrones estratégicos durante el entrenamiento, lo que le permitió tomar mejores decisiones en comparación con su contraparte. Sin embargo, al estar ambos agentes basados en la misma técnica, la diferencia fue menor que contra Minimax."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
